{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JGr-h1xvTOZ"
   },
   "source": [
    "# MNIST Simple Pointer Network\n",
    "\n",
    "In this notebook we will learn how to create a simple [Pointer Network](https://arxiv.org/abs/1506.03134) (Vinyals et al. 2015) for solving a dummy task on the MNIST dataset.\n",
    "\n",
    "A Pointer Network uses the **attention mechanism’s** output to model the conditional probability of each element on its input. This can be extremely useful in tasks that require selecting one (or more) elements of the input sequence/set to be solved.\n",
    "\n",
    "In this notebook we will play with a simple Pointer net to solve the following task: Given an MNIST image with a (query) digit we want our model to find the image that contains the consecutive digit to the query image among a set of input images. For example, imagine we use an image with the digit \"5\" as query and let the input of the model be a set of 10 images with different digits, the output of our model must be a probability distribution over those 10 images indicating the presence or not of the digit \"6\" in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4Z7-Wm7_5K6",
    "outputId": "9dbf2711-26a0-4bfc-e193-3f4c233d447e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNJh2q4lQGYF"
   },
   "outputs": [],
   "source": [
    "#If this cell fails you need to change the runtime of your colab notebook to GPU\n",
    "# Go to Runtime -> Change Runtime Type and select GPU\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyM0DXter9Mp"
   },
   "source": [
    "### Create the dataset\n",
    "\n",
    "Let's start creating a Dataset class to understand better the task we want to solve. Each sample in out dataset will be formed by:\n",
    "\n",
    "* a query image\n",
    "* a set of 10 images, 9 of them selected randomly (distractors) and one selected as the consecutive digit of the digit in the query image.\n",
    "* the ground truth: a 10-D one hot vector indicating the position of the image we want our model to select (\"point to\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XksdVS7GHWw0"
   },
   "outputs": [],
   "source": [
    "class MNISTAttentionDataset(Dataset):\n",
    "    \"\"\"MNIST attention toy dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_inputs (int) - The number of images in the input of our model.\n",
    "            train (bool, optional) – If True, creates dataset from MNIST training\n",
    "                samples, otherwise from test\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.mnist = torchvision.datasets.MNIST('../data', train=train, download=True)\n",
    "        \n",
    "        # dict with samples for each class label\n",
    "        self.data = {}\n",
    "        for label in range(10):\n",
    "            #Collect in separate lists the different digits, and normalise values to [0, 1]\n",
    "            self.data[label] = self.mnist.data[self.mnist.targets == label] / 255.0\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # The length makes little sense here, as we generate samples dynamically\n",
    "        # Here we return the number of images originally in the dataset\n",
    "        return self.mnist.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int) - an index indicating which digit should be used as query\n",
    "        \"\"\"\n",
    "        c = self.mnist.targets[idx].item()     # Find out which is the class of the digit requested\n",
    "        query = self.mnist.data[idx].flatten() / 255.0 # Get the data for the digit requested\n",
    "\n",
    "        correct_pos = np.random.randint(0, self.num_inputs) # Decide on a random position where the correct answer will be\n",
    "        \n",
    "        inputs = np.zeros((self.num_inputs, 784)) # Create an array to hold the inputs (from which we have to select one)\n",
    "        for j in range(self.num_inputs):\n",
    "            if j == correct_pos: # In the correct position, place a random digit that comes from the c+1 class\n",
    "                idx = np.random.randint(0,self.data[(c+1)%10].shape[0])\n",
    "                inputs[j,:] = self.data[(c+1)%10][idx,:].flatten()\n",
    "            else: # In the rest of the positions, place random digits that come from the rest of the classes\n",
    "                c_distractor = np.random.choice([n for n in [0,1,2,3,4,5,6,7,8,9] if n!=(c+1)%10])\n",
    "                idx = np.random.randint(0,self.data[c_distractor].shape[0])\n",
    "                inputs[j,:] = self.data[c_distractor][idx,:].flatten()\n",
    "            \n",
    "        sample = {'x': inputs, 'query': query, 'y': correct_pos}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL2kwN-kpjUc"
   },
   "outputs": [],
   "source": [
    "num_inputs = 10\n",
    "\n",
    "train_dataset = MNISTAttentionDataset(num_inputs, train=True)\n",
    "test_dataset = MNISTAttentionDataset(num_inputs, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size = 1000, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RES5R0CMr3bE"
   },
   "source": [
    "### Visualize one training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "fJY6zR_lqR41",
    "outputId": "789330a8-92bb-4445-dc72-9c4635342ecc"
   },
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "print(f\"Data: {sample['x'].shape}, Query: {sample['query'].shape}\")\n",
    "\n",
    "fig = plt.figure(figsize=(33, 3))\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = plt.subplot(1, num_inputs + 1, 1)\n",
    "image = sample['query'].reshape(28,28)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.plot([0,27,27,0,0], [0,0,27,27,0], c='b') # Draw a blue box around the query sample\n",
    "ax.set_title('Query')\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(num_inputs):\n",
    "    ax = plt.subplot(1, num_inputs + 1, i+2)\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    image = sample['x'][i].reshape(28,28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    if i == sample['y']:\n",
    "        plt.plot([0,27,27,0,0], [0,0,27,27,0], c='g') # Draw a green box around the correct sample\n",
    "        ax.set_title('Target')\n",
    "    else:\n",
    "        plt.plot([0,27,27,0,0], [0,0,27,27,0], c='r') # Draw a blue box around the wrong sample\n",
    "        ax.set_title('Distractor #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVaMM-n2sFF6"
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BesD8B1BAW9u"
   },
   "outputs": [],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_inputs, hidden_dim):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_inputs = num_inputs\n",
    "        self.input_dim  = input_dim\n",
    "\n",
    "        # Weights (Fully Connected layers) \n",
    "        self.fc_q = nn.Linear(input_dim, hidden_dim) # Query FC\n",
    "        self.fc_k = nn.Linear(input_dim, hidden_dim) # Keys FC. Here the input acts as the keys, and there is no value\n",
    "        self.fc_v = nn.Linear(hidden_dim, 1) # Values FC\n",
    "\n",
    "    def scoringAdditive(self, query, keys):\n",
    "        # Query is                                              (B X 1 X input_dim)\n",
    "        # Repeat (tile) the query so that it has the same size as the keys (input)\n",
    "        query = query.repeat(1, self.num_inputs, 1)           # (B X num_inputs X input_dim)\n",
    "        query = torch.tanh(self.fc_q(query))                  # (B X num_inputs X hidden_dim)\n",
    "\n",
    "        # Keys is                                               (B X num_inputs X input_dim)\n",
    "        keys = torch.tanh(self.fc_k(keys))                    # (B X num_inputs X hidden_dim)\n",
    "        \n",
    "        # Attention Score\n",
    "        score = torch.tanh(query + keys )                     # (B X num_inputs X hidden_dim)\n",
    "        score = self.fc_v(score)                              # (B X num_inputs X 1)        \n",
    "        return score\n",
    "\n",
    "    def forward(self, x, query):\n",
    "        query = query.unsqueeze(1) # (B x input_dim) -> (B x 1 x input_dim)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        output = self.scoringAdditive(query, x)\n",
    "        output = output.squeeze()       # (B x 1 x num_inputs) -> (B x num_inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_yCyvgg9Pbj"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "**QUESTION**: The Additive score gives logits that we should pass through a softmax to get the final attention weights. Why is there no softmax in the model above?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    \n",
    "**ANSWER**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsyUZE0lHJ5-",
    "outputId": "218369fc-efcb-4a62-9e29-5af570371c53"
   },
   "outputs": [],
   "source": [
    "input_dim, num_inputs, hidden_dim = (784, 10, 256)\n",
    "\n",
    "model = AttentionModel(input_dim, num_inputs, hidden_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmp0kuk2z3Rx"
   },
   "source": [
    "### Define the training method and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZvGWqwDrc-V"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch, log_interval = 100):\n",
    "    # Method to perform one epoch of training\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    \n",
    "    for batch_idx, sample_batched in enumerate(train_loader):\n",
    "        data = sample_batched['x'].float()\n",
    "        query = sample_batched['query'].float()\n",
    "        target = sample_batched['y']\n",
    "        #print(data.shape, query.shape)\n",
    "        data, query, target = data.to(device), query.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, query)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_values.append(loss.detach().cpu().item())        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_values = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    loss_history += loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jONlgxyaEwC6"
   },
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izgtCdk5DV01"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total = 0.\n",
    "    ok = 0.\n",
    "    for batch_idx, sample_batched in enumerate(test_loader):\n",
    "        data = sample_batched['x'].float()\n",
    "        query = sample_batched['query'].float()\n",
    "        target = sample_batched['y'].numpy()\n",
    "        data, query = data.to(device), query.to(device)\n",
    "        \n",
    "        output = model(data, query)\n",
    "        pred = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "       \n",
    "        ok += np.sum(pred == target)\n",
    "        total += len(test_loader)\n",
    "                \n",
    "    print('Accuracy = ', ok/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaOoDxjhhjxz"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "**EXERCISE 1**: Starting from the previous model as a template, implement the Dot-product Attention scoring function and substitute the additive one we were using before. Retrain and evalute the model.</font>\n",
    "\n",
    "<font color=\"blue\">\n",
    "\n",
    "> Hint: To transpose a tensor along specific dimensions you can use `torch.permute()`\n",
    "</font>\n",
    "\n",
    "\n",
    "<font color=\"blue\">\n",
    "\n",
    "> Hint: To perform a batch matrix-matrix product, use the function `torch.bmm()` https://pytorch.org/docs/stable/generated/torch.bmm.html \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MNIST_PointerNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
